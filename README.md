# MediaConfidant - Analytics Pipeline as Code for Data Products

[![Mutable.ai Auto Wiki](https://img.shields.io/badge/Auto_Wiki-Mutable.ai-blue)](https://wiki.mutable.ai/wrenchchatrepo/mediaconfidant)

## Project Overview
This project integrates advertising data from various platforms using BigQuery, Dataform, and Looker to provide comprehensive insights into ad performance. This repository contains the code and documentation for an analytics pipeline built for MediaConfidant using Google Cloud Workflows and Pulumi and other Google Cloud services. This pipeline automates data ingestion, transformation, analysis, and reporting to provide actionable insights for MediaConfidant's business. 

![MediaConfidant: Analytics Pipeline as Code on GCP](https://github.com/wrenchchatrepo/mediaconfidant/assets/158282973/2a86eb68-5be5-4714-84c4-571f2e256f82)

### Proposal for a Data Product Using Dataform, Looker, and BigQuery Machine Learning

The Data Product is a result of the Analytics Pipeline: the data model includes a semantic layer and data
transformations both with version control, sophisticated analytics, practical ML, and data integrity. This solution is
intended to be a blueprint for the customer MediaConfidant to build on Google Cloud. Infrastructure and Analytics as
Code means instructions are provided for provisioning the infrastructure, code snippets as examples, and thorough
documentation for every step.

The modern data stack is a collection of tools that work together to help businesses collect, clean, analyze, and
visualize data. Dataform, Looker, and BigQuery create a powerful analytical stack but are only part of the solution.

This document outlines the necessary supporting infrastructure with the following objectives:
+ Improved data insights: BigQuery's analytics engine can be used with Looker to produce more than just data.
+ Improved data quality: Dataform can be used to clean and validate data before it is loaded into BigQuery.
+ Increased data accessibility: Looker provides a user-friendly interface for users to access and explore data.
+ Data integration: Dataform will be used to integrate data from multiple sources into BigQuery.

### Architecture
This project utilizes a modular workflow approach to manage the entire data pipeline. Each workflow
represents a distinct stage of the process, ensuring a clear and organized structure.

### Data Sources
	Google Ads
	Facebook Ads
	Bing Ads
	TikTok Ads
	Google Analytics

## Analytics Engineering
1. Data Ingestion:
+ Data from various sources like BingAds and TikTok Ads is ingested into BigQuery then moved to Cloud Storage.
+ Google Analytics data is exported to BigQuery ML.
2. Data Processing:
+ BigQuery ML processes data from Cloud Storage and other sources.
+ DataForm integrates with GitHub for version control and uses Cloud Functions for automation.
+ SQLX is used within DataForm for data transformations.
3. Data Model:
+ Looker is used for data visualization and reporting.
+ LookML integrates with SQLX to add relational integrity.
+ Spectacles is recommendaed for LookML and content validation.
4. Outputs
+ Dashboards: Generated by Looker for data visualization.
+ Looker Actions: Custom actions integrated into Looker.
+ Looker API: Used for programmatically accessing Looker data.
+ Looker SDK: For integrating Looker with other applications.
+ Cloud Marketplace: For publishing and accessing Looker applications.

## Infrastructure as Code

### Cloud Environment
+ Docker: Containerization for applications.
+ Resource Manager: For managing cloud resources.
+ Cloud APIs: Various APIs provided by Google Cloud.
+ Billing: Managing billing and costs.
+ VPC: Virtual Private Cloud for networking.
+ Firewall: Security and access control.
+ Workflows: Orchestrating complex workflows.
+ Cloud SDK: Tools and libraries for cloud management.
+ Deployment API: Automated deployment of applications.
+ Cloud Build: CI/CD for building and deploying applications.
+ Vertex AI: For machine learning and AI applications.
+ Cloud Logging: Centralized logging for monitoring and troubleshooting.

### Getting Started
To get started with the project, follow these steps:
1. Set up your environment:
+ Ensure you have the required tools installed, such as Docker, Cloud SDK, and other dependencies.
+ Follow the setup instructions in the relevant directories.
2. Deploy the infrastructure:
+ Use Pulumi Go Commands to set up the infrastructure on GCP.
+ Ensure all services are correctly configured and accessible.
3. Run the data pipeline:
+ Use DataForm to execute the data ingestion and transformation processes.
+ Monitor the pipeline using Cloud Logging and Cloud Monitoring.

### Issues
<!-- issueTable -->
| Title                                                                                                  |         Status          | Assignee | Body           |
| :----------------------------------------------------------------------------------------------------- | :---------------------: | :------: | :------------- |
| <a href="https://github.com/wrenchchatrepo/orale_customer/issues/1">Just a test of a github action</a> | :eight_spoked_asterisk: |          | Please ignore. |
<!-- issueTable -->

### Directory Structure
```
/dataform
	/definitions
	standardized_ads_data.sqlx
	combined_metrics.sqlx
	/dataform.json
/bigquery
	google_ads_schema.sql
	facebook_ads_schema.sql
	bing_ads_schema.sql
	tiktok_ads_schema.sql
	google_analytics_schema.sql
/looker
	/views
	standardized_ads_data.view.lkml
	combined_metrics.view.lkml
	logistic_regression_predictions.view.lkml
	arima_plus_forecasts.view.lkml
	/models
	data_product.model.lkml
	/manifest
	manifest.lkml
	/dashboards
	combined_metrics_dashboard.dashboard.lkml
	logistic_regression_predictions_dashboard.dashboard.lkml
	arima_plus_forecasts_dashboard.dashboard.lkml
/scripts
	create_standardized_ads_data_view.py
	create_combined_metrics_view.py
	create_logistic_regression_predictions_view.py
	create_arima_plus_forecasts_view.py
	create_model_file.py
	create_manifest_file.py
	create_combined_metrics_dashboard.py
	create_logistic_regression_predictions_dashboard.py
	create_arima_plus_forecasts_dashboard.py
```

## The provided document is a comprehensive plan for integrating and analyzing advertising data from various platforms using BigQuery, Dataform, and Looker. It covers the following key areas:
1. Metrics Identification:
+ Detailed metrics for Google Analytics, Google Ads, Bing Ads, Facebook Ads, Instagram Ads, and TikTok Business Ads.
+ Combining these metrics to create new metrics like Ad Conversion Rate, Ad ROI, Session Value, Ad Engagement Rate, Bounce Rate per Ad Channel, Cross-Channel Conversion Rate, Average Session Duration per Ad Channel, and Ad Cost per Acquisition (CPA).
2. Google Analytics Export to BigQuery:
+ Steps to set up BigQuery linking and data export.
+ Schema and structure of exported data.
3. Data Integration from Various Platforms:
+ Instructions for setting up data transfers for Google Ads and Facebook Ads using BigQuery Data Transfer Service.
+ API calls for Bing Ads and TikTok Ads, including authentication and fetching ad data.
+ Loading data into BigQuery tables and transforming the data using Dataform SQLX.
4. Estimating Data Volume and Storage:
+ Estimations for the amount of data pulled daily and weekly.
+ Calculations based on the number of campaigns, data points per record, and row sizes.
5. BigQuery Table Schemas:
+ Schemas for raw data tables for each ad platform and Google Analytics.
+ Transformed data tables like standardized_ads_data and combined_metrics.
6. LookML Implementation:
+ View files for standardized_ads_data and combined_metrics.
+ Model file for defining explores and joins.
+ Manifest file for defining the dashboards.
7. Looker Dashboards:
+ Dashboards for combined_metrics, logistic_regression_predictions, and arima_plus_forecasts.
8. Row-Level Security with Looker User Attributes:
+ Using user attributes to securely deliver data to customers through the Looker UI.
9. BigQuery ML Models:
+ Logistic Regression for binary classification and ARIMA_PLUS for time series forecasting.
+ SQL scripts for data preparation, training models, and making predictions.

## Summary of Key Code Sections
### View Files
+ standardized_ads_data.view.lkml: Defines the schema for standardized ads data.
+ combined_metrics.view.lkml: Defines the schema + logistic_regression_predictions.view.lkml: Defines the schema for logistic regression predictions.
+ arima_plus_forecasts.view.lkml: Defines the schema for ARIMA_PLUS forecasts.
### Model File
+ data_product.model.lkml: Includes views and defines explores for data_product_explore.
### Manifest File
+ manifest.lkml: Defines the project, models, and dashboards.
### Dashboards
+ combined_metrics_dashboard.dashboard.lkml: Dashboard for combined metrics.
+ logistic_regression_predictions_dashboard.dashboard.lkml: Dashboard for logistic regression predictions.
+ arima_plus_forecasts_dashboard.dashboard.lkml: Dashboard for ARIMA_PLUS forecasts.
### Metrics by Data Source
| Metric                   | Google Analytics | Google Ads | Bing Ads | Facebook Ads | Instagram Ads | TikTok Business Ads |
|--------------------------|------------------|------------|----------|--------------|---------------|---------------------|
| Users                    | X                |            |          |              |               |                     |
| Sessions                 | X                |            |          |              |               |                     |
| Bounce Rate              | X                |            |          |              |               |                     |
| Average Session Duration | X                |            |          |              |               |                     |
| Pageviews                | X                |            |          |              |               |                     |
| Clicks                   |                  | X          | X        | X            | X             |                     |
| Impressions              |                  | X          | X        | X            | X             | X                   |
| CTR (Click-Through Rate) |                  | X          | X        | X            | X             | X                   |
| CPC (Cost Per Click)     |                  | X          | X        | X            | X             |                     |
| Conversions              |                  | X          | X        |              |               | X                   |
| Reach                    |                  |            |          | X            | X             |                     |
| Engagement               |                  |            |          | X            | X             |                     |
| Video Views              |                  |            |          |              |               | X                   |
| Engagement Rate          |                  |            |          |              |               | X                   |
| Conversion Rate          |                  |            |          |              |               | X                   |
### Google Analytics
1. Users. The number of unique visitors to your website.
2. Sessions. A group of interactions that take place on your website within a given time frame.
3. Bounce Rate. The percentage of single-page sessions where there was no interaction with the page.
4. Average Session Duration. The average length of a session on your website.
5. Pageviews. The total number of pages viewed. Repeated views of a single page are counted.
### Google Ads
1. Clicks. The number of times users clicked on your ad.
2. Impressions. The number of times your ad was shown on a search result page or other site on the Google Network.
3. CTR (Click-Through Rate). The ratio of users who click on your ad to the number of total users who view your ad.
4. CPC (Cost Per Click). The average cost you pay for each click on your ad.
5. Conversions. The number of times users completed a desired action (e.g., purchase, sign-up) after clicking on your ad.
### Bing Ads
1. Clicks. The number of clicks your ads receive.
2. Impressions. The number of times your ads are displayed on Bing Network search results.
3. CTR (Click-Through Rate). The percentage of people who clicked your ad after seeing it.
4. CPC (Cost Per Click). The amount you pay for each click on your ad.
5. Conversions. Actions counted when someone interacts with your ad (e.g., clicks it) and then takes an action you’ve defined as valuable to your business.
### Meta Ads (Facebook, Instagram)
1. Reach. The number of unique users who have seen your ad.
2. Impressions. The number of times your ads were on screen.
3. Engagement. The total number of actions (likes, comments, shares) people take involving your ads.
4. CTR (Click-Through Rate). The ratio of users who click on your ad to the number of total users who view your ad.
5. CPC (Cost Per Click). The amount you pay for each click on your ad.
### TikTok Business Ads
1. Video Views. The number of times your video ad was viewed.
2. Impressions. The number of times your ad was shown on TikTok.
3. CTR (Click-Through Rate). The percentage of people who clicked your ad after seeing it.
4. Engagement Rate. The ratio of engagements (likes, comments, shares) to impressions.
5. Conversion Rate. The percentage of users who completed a desired action (e.g., sign-up, purchase) after interacting with your ad.
### Combined Metrics
Combining metrics from Google Analytics with those from advertising platforms like Google Ads, Bing Ads, Facebook Ads, Instagram Ads, and TikTok Business Ads can provide deeper insights into the effectiveness of ad campaigns and their impact on website performance. Here are a few new metrics, their calculations, and the insights they could provide:

1. Ad Conversion Rate
+ Calculation:
\text{Ad Conversion Rate} = \frac{\text{Conversions (Ad Platform)}}{\text{Clicks (Ad Platform)}} \times 100
+ This metric shows the effectiveness of an ad in driving users to complete a desired action (e.g., purchase, sign-up). A high conversion rate indicates that the ad is highly relevant and persuasive.

2. Ad ROI (Return on Investment)
+ Calculation:
\text{Ad ROI} = \frac{\text{Revenue (Google Analytics)}}{\text{Cost (Ad Platform)}} \times 100
+ This metric measures the profitability of the ad campaign. A high ROI indicates that the campaign is generating significant revenue relative to its cost.

3. Session Value
+ Calculation:
\text{Session Value} = \frac{\text{Revenue (Google Analytics)}}{\text{Sessions (Google Analytics)}}
+ This metric provides an average value of each session on the website. It helps in understanding how much revenue each visit brings, which can guide budget allocation for ad campaigns.

4. Ad Engagement Rate
+ Calculation:
\text{Ad Engagement Rate} = \frac{\text{Engagements (Ad Platform)}}{\text{Impressions (Ad Platform)}} \times 100
+ This metric shows how engaging the ad is to the audience. A high engagement rate indicates that the ad content resonates well with the viewers.

5. Bounce Rate per Ad Channel
+ Calculation:
\text{Bounce Rate per Ad Channel} = \frac{\text{Single-Page Sessions (Google Analytics)}}{\text{Sessions (Google Analytics)}} \times 100 \quad \text{(for traffic from each ad channel)}
+ This metric helps in understanding the quality of traffic from each ad channel. A high bounce rate indicates that users are not finding what they expected on the landing page.

6. Cross-Channel Conversion Rate
+ Calculation:
\text{Cross-Channel Conversion Rate} = \frac{\text{Conversions (Google Analytics)}}{\text{Total Clicks from All Ad Platforms}} \times 100
+ This metric provides an overall view of how effective all ad campaigns combined are in driving conversions. It helps in assessing the combined impact of multi-channel ad strategies.

7. Average Session Duration per Ad Channel
+ Calculation:
\text{Average Session Duration per Ad Channel} = \frac{\text{Total Session Duration (Google Analytics)}}{\text{Sessions (Google Analytics)}} \quad \text{(for traffic from each ad channel)}
+ This metric helps in understanding how engaged users from different ad channels are. Longer session durations indicate higher engagement levels.

8. Ad Cost per Acquisition (CPA)
+ Calculation:
\text{Ad CPA} = \frac{\text{Cost (Ad Platform)}}{\text{Conversions (Google Analytics)}}
+ This metric shows the cost-effectiveness of ad campaigns in acquiring customers. A lower CPA indicates a more cost-efficient campaign.

### Example Names and Insights for New Metrics:
1. Ad Conversion Rate: Measures the efficiency of ad campaigns in converting clicks to actions.
2. Ad ROI: Assesses the profitability of ad campaigns.
3. Session Value: Indicates the average revenue per session.
4. Ad Engagement Rate: Evaluates the level of engagement generated by ads.
5. Bounce Rate per Ad Channel: Analyzes the quality of traffic from different ad channels.
6. Cross-Channel Conversion Rate: Provides a holistic view of multi-channel ad effectiveness.
7. Average Session Duration per Ad Channel: Understands user engagement from various ad sources.
8. Ad Cost per Acquisition (CPA): Determines the cost efficiency in acquiring new customers.

## Setting Up Export to BigQuery

Google Analytics’ Export to BigQuery feature allows you to export raw event data from Google Analytics to Google BigQuery, enabling deeper and more flexible analysis using SQL queries. Here’s how it works:
1. Enable BigQuery Linking:
+ Go to the Google Analytics Admin section.
+ In the “Property” column, click “BigQuery Linking.”
+ Click “+ Link” to create a new link between Google Analytics and BigQuery.
+ Follow the setup wizard, selecting the BigQuery project and dataset where you want the data to be exported.
2. Configure Data Export:
+ Choose the data streams (web and/or app) that you want to export.
+ Set the export frequency (daily or streaming export).
+ Review and complete the setup.

When the data is exported, it is stored in a specific structure within BigQuery:
+ Dataset: The data is stored in a BigQuery dataset.
+ Tables: The dataset contains tables for each day of data. For example, a table for data collected on June 1st, 2024, would be named events_20240601.

The schema of the exported data includes the following fields:
1. event_date: The date on which the event occurred.
2. event_timestamp: The timestamp of the event.
3. event_name: The name of the event.
4. user_id: The unique identifier for the user.
5. user_properties: Custom properties associated with the user.
6. event_params: Parameters associated with the event.
7. geo: Geographical information about the user.
8. app_info: Information about the app where the event occurred.
9. traffic_source: Information about the traffic source.

Once the data is in BigQuery, you can use SQL queries to analyze it. For example:
```
-- Example query to get the number of users per event type
SELECT
  event_name,
  COUNT(DISTINCT user_id) AS user_count
FROM
  `project_id.dataset_id.events_*`
WHERE
  _TABLE_SUFFIX BETWEEN '20240601' AND '20240630'
GROUP BY
  event_name
ORDER BY
  user_count DESC;
```

### Benefits of Exporting to BigQuery
+ Advanced Analysis: Perform complex queries and analysis that are not possible in the Google Analytics interface.
+ Integration: Integrate Google Analytics data with other datasets in BigQuery for a comprehensive view of your data.
+ Custom Reporting: Create custom reports and dashboards using tools like Google Data Studio, directly connected to BigQuery.

### Considerations
+ Cost: BigQuery charges for data storage and queries, so it’s important to monitor usage and costs.
+ Data Freshness: Depending on your configuration, there may be a delay between data 

### Integrating the Data
To integrate data from Google Ads, Facebook Ads, TikTok, and Bing Ads with Google Analytics data in BigQuery, you’ll need to use a combination of data transfers, API integrations, and data transformations. Here’s a step-by-step guide on how to achieve this integration and perform necessary transformations using Dataform:

### Step-by-Step Guide
1. Setting Up Data Transfers and API Integrations for Google Ads and Facebook Ads
+ Google Ads:
+ Use the BigQuery Data Transfer Service to set up a transfer from Google Ads.
+ In the Google Cloud Console, navigate to BigQuery and select “Transfers.”
+ Create a new transfer and select “Google Ads” as the data source.
+ Follow the setup wizard to authorize and configure the transfer.
+ Facebook Ads:
+ Similar to Google Ads, use the BigQuery Data Transfer Service.
+ Select “Facebook Ads” as the data source and follow the setup wizard.
2. TikTok and Bing Ads API Integration:
+ TikTok Ads:
+ Use the TikTok for Business API to fetch ad data.
+ Implement a scheduled Cloud Function or a Cloud Run service to call the API and store the data in BigQuery.
+ Bing Ads:
+ Use the Bing Ads API to fetch ad data.
+ Similar to TikTok, use a scheduled Cloud Function or Cloud Run to call the API and load data into BigQuery.

Bing Ads API Calls

1. Authentication:
+ Use OAuth2 to get an access token.
+ Example using Python and requests library:
```
import requests

client_id = 'YOUR_CLIENT_ID'
client_secret = 'YOUR_CLIENT_SECRET'
refresh_token = 'YOUR_REFRESH_TOKEN'

token_url = 'https://login.microsoftonline.com/common/oauth2/v2.0/token'
headers = {'Content-Type': 'application/x-www-form-urlencoded'}
data = {
    'client_id': client_id,
    'client_secret': client_secret,
    'refresh_token': refresh_token,
    'grant_type': 'refresh_token',
    'scope': 'https://ads.microsoft.com/.default'
}

response = requests.post(token_url, headers=headers, data=data)
access_token = response.json().get('access_token')
```

2.	Fetching Ad Data:
+ Use the Reporting API to get performance data.
+ Example for fetching a report:
```
report_request = {
    "ReportRequest": {
        "ReportName": "AdPerformanceReport",
        "Format": "Csv",
        "ReportType": "AdPerformanceReport",
        "ReturnOnlyCompleteData": False,
        "Aggregation": "Daily",
        "Scope": {
            "AccountIds": [YOUR_ACCOUNT_ID]
        },
        "Time": {
            "CustomDateRangeStart": {
                "Year": 2023,
                "Month": 1,
                "Day": 1
            },
            "CustomDateRangeEnd": {
                "Year": 2023,
                "Month": 1,
                "Day": 31
            }
        },
        "Columns": [
            "TimePeriod",
            "AccountId",
            "CampaignId",
            "AdId",
            "Impressions",
            "Clicks",
            "Spend",
            "Conversions"
        ]
    }
}

headers = {
    'Authorization': f'Bearer {access_token}',
    'Content-Type': 'application/json'
}

report_url = 'https://api.bingads.microsoft.com/v13/Reporting/SubmitGenerateReport'
response = requests.post(report_url, headers=headers, json=report_request)
report_id = response.json().get('ReportRequestId')

# Check report status and download once ready
```
TikTok Ads API Calls

1. Authentication:
+ Use OAuth2 to get an access token.
+ Example using Python and requests library:
```
import requests

client_key = 'YOUR_CLIENT_KEY'
client_secret = 'YOUR_CLIENT_SECRET'
auth_code = 'YOUR_AUTH_CODE'

token_url = 'https://business-api.tiktok.com/open_api/v1.2/oauth2/access_token/'
headers = {'Content-Type': 'application/json'}
data = {
    'app_id': client_key,
    'secret': client_secret,
    'auth_code': auth_code,
    'grant_type': 'authorization_code'
}

response = requests.post(token_url, headers=headers, json=data)
access_token = response.json().get('data').get('access_token')
```
2.	Fetching Ad Data:
+ Use the Reporting API to get performance data.
+ Example for fetching a report:
```
report_request = {
    "advertiser_id": "YOUR_ADVERTISER_ID",
    "report_type": "BASIC",
    "dimensions": ["ad_id", "campaign_id", "date"],
    "metrics": ["impressions", "clicks", "spend", "conversions"],
    "start_date": "2023-01-01",
    "end_date": "2023-01-31",
    "page": 1,
    "page_size": 1000
}

headers = {
    'Access-Token': access_token,
    'Content-Type': 'application/json'
}

report_url = 'https://business-api.tiktok.com/open_api/v1.2/reports/integrated/get/'
response = requests.post(report_url, headers=headers, json=report_request)
report_data = response.json().get('data')
```

2. Loading Data into BigQuery
+ Ensure each data source (Google Ads, Facebook Ads, TikTok Ads, Bing Ads) is loaded into separate tables within a BigQuery dataset. For example:
+ google_ads_data
+ facebook_ads_data
+ tiktok_ads_data
+ bing_ads_data

Integrating Data with BigQuery

1. Load Data into BigQuery:
+ Use Google Cloud client libraries to load the fetched data into BigQuery.
+ Example using Python:
```
from google.cloud import bigquery

client = bigquery.Client()

dataset_id = 'your_project.your_dataset'
table_id = f'{dataset_id}.bing_ads_data'
job_config = bigquery.LoadJobConfig(
    schema=[
        bigquery.SchemaField("ad_id", "STRING"),
        bigquery.SchemaField("campaign_id", "STRING"),
        bigquery.SchemaField("date", "DATE"),
        bigquery.SchemaField("impressions", "INTEGER"),
        bigquery.SchemaField("clicks", "INTEGER"),
        bigquery.SchemaField("spend", "FLOAT"),
        bigquery.SchemaField("conversions", "INTEGER"),
    ],
    skip_leading_rows=1,
    source_format=bigquery.SourceFormat.CSV,
)

uri = "gs://your_bucket/bing_ads_report.csv"
load_job = client.load_table_from_uri(
    uri, table_id, job_config=job_config
)

load_job.result()
```
2. Transform Data Using Dataform:
+ Set up Dataform to perform transformations and combine the data with Google Analytics data.
```
config {
  type: "table",
  description: "Combined metrics with Google Analytics data"
}

select
  ads.ad_id,
  ads.campaign_id,
  ads.platform,
  ads.impressions,
  ads.clicks,
  ads.conversions,
  ads.spend as cost,
  ga.sessions,
  ga.bounce_rate,
  ga.avg_session_duration,
  ga.pageviews,
  ads.spend / ads.conversions as cpa,
  ga.revenue
from ${ref("standardized_ads_data")} as ads
left join ${ref("google_analytics_data")} as ga
on ads.timestamp = ga.date and ads.campaign_id = ga.campaign_id
```

3. Transforming Data Using Dataform
+ Use Dataform to define transformations and create a unified view of your ad data.

Example Dataform Project Structure
+ project.sqlx
+ Contains the main transformations and definitions.

Example SQLX Transformations
1. Create Unified Ad Table:
```
config {
  type: "table",
  description: "Unified ad data from multiple sources"
}
select * from ${ref("google_ads_data")}
union all
select * from ${ref("facebook_ads_data")}
union all
select * from ${ref("tiktok_ads_data")}
union all
select * from ${ref("bing_ads_data")}
```
2. Transformations to Align Data:
+ Ensure columns from different data sources are named consistently.
+ Example transformation to standardize column names:
```
config {
  type: "table",
  description: "Standardized ad data"
}

select
  ad_id,
  campaign_id,
  source as platform,
  cast(impressions as int64) as impressions,
  cast(clicks as int64) as clicks,
  cast(conversions as int64) as conversions,
  cast(cost as float64) as cost,
  parse_timestamp("%Y-%m-%dT%H:%M:%S%z", timestamp) as timestamp
from ${ref("unified_ads_data")}
```
3.	Join with Google Analytics Data:
```
config {
  type: "table",
  description: "Combined metrics with Google Analytics data"
}
select
  ads.ad_id,
  ads.campaign_id,
  ads.platform,
  ads.impressions,
  ads.clicks,
  ads.conversions,
  ads.cost,
  ga.sessions,
  ga.bounce_rate,
  ga.avg_session_duration,
  ga.pageviews,
  ads.cost / ads.conversions as cpa,
  ga.revenue
from ${ref("standardized_ads_data")} as ads
left join ${ref("google_analytics_data")} as ga
on ads.timestamp = ga.date and ads.campaign_id = ga.campaign_id
```
The amount of data pulled daily and the intervals can vary based on several factors, including the volume of ad activity, the number of campaigns, and the specific requirements of your business. Here’s a general guideline for each vendor:

Google Ads and Facebook Ads (via BigQuery Data Transfer Service)
+ Data Volume: The volume of data can range from a few megabytes to several gigabytes per day, depending on the number of ad impressions, clicks, and conversions.
+ Interval: The data transfer service typically pulls data once daily. This is usually set up to run during off-peak hours to minimize any impact on performance.

Bing Ads and TikTok Ads (via API)
For both Bing Ads and TikTok Ads, you will need to set up custom scripts or functions to pull data. The volume and frequency will depend on your ad activity and the API rate limits.

Bing Ads
+ Data Volume: Similar to Google Ads and Facebook Ads, it can range from megabytes to gigabytes per day.
+ Interval: You can schedule data pulls at intervals that balance the need for up-to-date data with API rate limits and performance considerations. Common intervals are:
+ Daily: Pulling data once every 24 hours, typically during off-peak hours.
+ Hourly: For more real-time insights, you can pull data every hour, but this increases the load on your API quota.

TikTok Ads
+ Data Volume: Similar to other ad platforms, expect data volume to range from megabytes to gigabytes per day.
+ Interval: Similar to Bing Ads:
+ Daily: Once every 24 hours is a common schedule.
+ Hourly: If real-time insights are critical, you can schedule more frequent pulls.

Estimating Data Volume

Here’s a rough estimation approach to understand the data volume:

1. Google Ads and Facebook Ads:
+ Impressions, Clicks, Conversions: Assume you have 10 campaigns, each generating 10,000 impressions, 1,000 clicks, and 50 conversions per day.
+ Data Points per Record: If each record (impression, click, conversion) has around 10 data points (e.g., timestamps, IDs, metrics), you have:
+ Impressions: 10,000 \times 10 = 100,000 data points
+ Clicks: 1,000 \times 10 = 10,000 data points
+ Conversions: 50 \times 10 = 500 data points
+ Total Data Points: 100,000 + 10,000 + 500 = 110,500 data points per campaign, per day.
+ Total Volume: With 10 campaigns, 110,500 \times 10 = 1,105,000 data points per day.
+ Storage Estimate: Assuming each data point is around 100 bytes, this translates to approximately 110.5 \text{MB} per day for 10 campaigns.
2. Bing Ads and TikTok Ads:
+ Use a similar approach to estimate based on the number of campaigns and activity levels.

Summary of Data Pull Intervals

+ Google Ads and Facebook Ads:
+ Frequency: Once daily
+ Volume: Typically megabytes to gigabytes, depending on ad activity.
+ Bing Ads:
+ Frequency: Daily or hourly, depending on requirements and API limits.
+ Volume: Similar to Google Ads, ranging from megabytes to gigabytes.
+ TikTok Ads:
+ Frequency: Daily or hourly, depending on requirements and API limits.
+ Volume: Similar to other ad platforms, ranging from megabytes to gigabytes.

Integrating and Transforming Data

After pulling the data:

1. Store Data in BigQuery: Each API pull stores data in designated tables within your BigQuery dataset.
2. Transform Using Dataform: Schedule Dataform transformations to standardize and combine data from different sources.
3. Create Combined Metrics: Use SQL and JavaScript in Dataform to compute combined metrics, ensuring that transformations run after data pulls to keep data fresh.

Monitoring and Optimization

+ API Rate Limits: Monitor API usage to avoid exceeding rate limits.
+ Data Freshness: Ensure that data pull schedules and transformations align to keep insights up-to-date.
+ Cost Management: Monitor storage and query costs in BigQuery, optimizing data pulls and transformations to balance freshness with cost efficiency.

By setting up this process, you can efficiently integrate and analyze ad data from multiple platforms, enabling comprehensive and actionable insights.

Estimating the number of rows, columns, and storage in megabytes per day for each vendor involves making assumptions based on available public data and typical usage patterns. Here’s a detailed estimate for each vendor:

Assumptions

1. Average Columns per Record:
+ Each ad platform typically tracks metrics such as impressions, clicks, conversions, cost, ad ID, campaign ID, timestamp, and more.
+ Assume each record has about 15 columns.
2. Average Row Size:
+ Assume each column has an average size of 50 bytes (including overhead).
+ Each row would be approximately 15 columns * 50 bytes = 750 bytes.
3. Number of Campaigns:
+ Assume an average company runs 20 campaigns per platform.
4. Daily Metrics per Campaign:
+ Impressions: 20,000
+ Clicks: 2,000
+ Conversions: 100
5. Data Points per Campaign:
+ Each impression, click, and conversion is a separate data point.

Estimates for Each Vendor

Google Ads

+ Rows per Day:
+ Impressions: 20 \text{ campaigns} \times 20,000 \text{ impressions}
+ Clicks: 20 \text{ campaigns} \times 2,000 \text{ clicks}
+ Conversions: 20 \text{ campaigns} \times 100 \text{ conversions}
+ Total Rows: 20 \times (20,000 + 2,000 + 100) = 440,000 rows
+ Columns per Row: 15
+ Row Size: 750 bytes
+ Daily Storage: 440,000 \times 750 = 330,000,000 bytes ≈ 330 MB

Facebook Ads

+ Rows per Day:
+ Similar to Google Ads
+ Total Rows: 440,000 rows
+ Columns per Row: 15
+ Row Size: 750 bytes
+ Daily Storage: 440,000 \times 750 = 330,000,000 bytes ≈ 330 MB

Bing Ads

+ Rows per Day:
+ Similar to Google Ads
+ Total Rows: 440,000 rows
+ Columns per Row: 15
+ Row Size: 750 bytes
+ Daily Storage: 440,000 \times 750 = 330,000,000 bytes ≈ 330 MB

TikTok Ads

+ Rows per Day:
+ Similar to Google Ads
+ Total Rows: 440,000 rows
+ Columns per Row: 15
+ Row Size: 750 bytes
+ Daily Storage: 440,000 \times 750 = 330,000,000 bytes ≈ 330 MB

Total Storage Estimate per Day

Summing up the daily storage for all platforms:

+ Google Ads: 330 MB
+ Facebook Ads: 330 MB
+ Bing Ads: 330 MB
+ TikTok Ads: 330 MB
+ Total Daily Storage: 4 \times 330 MB = 1320 MB ≈ 1.32 GB

Total Monthly Storage Estimate

Assuming 30 days in a month:

+ Total Monthly Storage: 1.32 \text{ GB/day} \times 30 \text{ days} = 39.6 GB

To effectively manage and analyze ad data from various vendors in BigQuery, you’ll create separate tables for each vendor’s raw data and additional tables for the transformed data. Here’s a detailed description of the tables:

Summary of Tables

1. Raw Data Tables:
+ google_ads_data
+ facebook_ads_data
+ bing_ads_data
+ tiktok_ads_data
+ google_analytics_data
2. Transformed Data Tables:
+ standardized_ads_data
+ combined_metrics

1. Google Ads Data Table
```
CREATE TABLE `project_id.dataset_id.google_ads_data` (
  ad_id STRING,
  campaign_id STRING,
  ad_group_id STRING,
  impressions INT64,
  clicks INT64,
  conversions INT64,
  cost FLOAT64,
  date DATE,
  account_id STRING,
  other_metrics JSON
);
```
2. Facebook Ads Data Table
```
CREATE TABLE `project_id.dataset_id.facebook_ads_data` (
  ad_id STRING,
  campaign_id STRING,
  ad_set_id STRING,
  impressions INT64,
  clicks INT64,
  conversions INT64,
  cost FLOAT64,
  date DATE,
  account_id STRING,
  other_metrics JSON
);
```
3. Bing Ads Data Table
```
CREATE TABLE `project_id.dataset_id.bing_ads_data` (
  ad_id STRING,
  campaign_id STRING,
  ad_group_id STRING,
  impressions INT64,
  clicks INT64,
  conversions INT64,
  cost FLOAT64,
  date DATE,
  account_id STRING,
  other_metrics JSON
);
```
4. TikTok Ads Data Table
```
CREATE TABLE `project_id.dataset_id.tiktok_ads_data` (
  ad_id STRING,
  campaign_id STRING,
  ad_group_id STRING,
  impressions INT64,
  clicks INT64,
  conversions INT64,
  cost FLOAT64,
  date DATE,
  account_id STRING,
  other_metrics JSON
);
```
Table for Google Analytics Data
```
CREATE TABLE `project_id.dataset_id.google_analytics_data` (
  date DATE,
  session_id STRING,
  user_id STRING,
  sessions INT64,
  bounce_rate FLOAT64,
  avg_session_duration FLOAT64,
  pageviews INT64,
  revenue FLOAT64,
  campaign_id STRING,
  other_metrics JSON
);
```
Tables for Transformed Data
1. Standardized Ads Data Table
```
CREATE TABLE `project_id.dataset_id.standardized_ads_data` (
  ad_id STRING,
  campaign_id STRING,
  platform STRING,
  impressions INT64,
  clicks INT64,
  conversions INT64,
  cost FLOAT64,
  date DATE
);
```
SQLX Transformation for Standardized Ads Data
```
config {
  type: "table",
  description: "Standardized ad data from multiple sources"
}

SELECT
  ad_id,
  campaign_id,
  'Google Ads' AS platform,
  impressions,
  clicks,
  conversions,
  cost,
  date
FROM ${ref("google_ads_data")}
UNION ALL
SELECT
  ad_id,
  campaign_id,
  'Facebook Ads' AS platform,
  impressions,
  clicks,
  conversions,
  cost,
  date
FROM ${ref("facebook_ads_data")}
UNION ALL
SELECT
  ad_id,
  campaign_id,
  'Bing Ads' AS platform,
  impressions,
  clicks,
  conversions,
  cost,
  date
FROM ${ref("bing_ads_data")}
UNION ALL
SELECT
  ad_id,
  campaign_id,
  'TikTok Ads' AS platform,
  impressions,
  clicks,
  conversions,
  cost,
  date
FROM ${ref("tiktok_ads_data")}
```
2. Combined Metrics Table
```
CREATE TABLE `project_id.dataset_id.combined_metrics` (
  ad_id STRING,
  campaign_id STRING,
  platform STRING,
  impressions INT64,
  clicks INT64,
  conversions INT64,
  cost FLOAT64,
  sessions INT64,
  bounce_rate FLOAT64,
  avg_session_duration FLOAT64,
  pageviews INT64,
  cpa FLOAT64,
  revenue FLOAT64,
  date DATE
);
```
SQLX Transformation for Combined Metrics
```
config {
  type: "table",
  description: "Combined metrics with Google Analytics data"
}

SELECT
  ads.ad_id,
  ads.campaign_id,
  ads.platform,
  ads.impressions,
  ads.clicks,
  ads.conversions,
  ads.cost,
  ga.sessions,
  ga.bounce_rate,
  ga.avg_session_duration,
  ga.pageviews,
  ads.cost / ads.conversions AS cpa,
  ga.revenue,
  ads.date
FROM ${ref("standardized_ads_data")} AS ads
LEFT JOIN ${ref("google_analytics_data")} AS ga
ON ads.date = ga.date AND ads.campaign_id = ga.campaign_id
```
We can combine these new metrics into a unified schema for a comprehensive view of ad performance. Here’s an example schema for the combined metrics table:
```
CREATE TABLE `project_id.dataset_id.combined_ad_metrics` (
  platform STRING,
  campaign_id STRING,
  ad_conversion_rate FLOAT64,
  ad_roi FLOAT64,
  session_value FLOAT64,
  ad_engagement_rate FLOAT64,
  bounce_rate_per_ad_channel FLOAT64,
  cross_channel_conversion_rate FLOAT64,
  avg_session_duration_per_ad_channel FLOAT64,
  ad_cpa FLOAT64,
  date DATE
);
```
To estimate the size, let’s assume the following:

1. Number of Campaigns: 20 campaigns per platform.
2. Number of Platforms: 4 platforms (Google Ads, Facebook Ads, Bing Ads, TikTok Ads).
3. Number of Rows per Day: One row per campaign per platform per day.
+ 20 \text{ campaigns} \times 4 \text{ platforms} = 80 \text{ rows/day}
4. Number of Rows per Week:
+ 80 \text{ rows/day} \times 7 \text{ days} = 560 \text{ rows/week}

Columns and Row Size

1. Columns: The combined table has 10 columns.
2. Row Size Estimation:
+ Assume each string column (platform, campaign_id) is about 20 bytes.
+ Each float column is 8 bytes.
+ Date column is 8 bytes.
+ Total row size: 20 \text{ bytes} \times 2 + 8 \text{ bytes} \times 7 + 8 \text{ bytes} = 96 \text{ bytes/row}

Total Storage Estimate for One Week

1. Number of Rows per Week: 560 rows
2. Total Storage:

560 \text{ rows} \times 96 \text{ bytes/row} = 53,760 \text{ bytes} = 53.76 \text{ KB}


Given the low volume, the data storage for one week is very small, approximately 54 KB.

Example SQL to Populate the Combined Metrics Table

Here’s an example of how you might populate this table with combined metrics data:
```
INSERT INTO `project_id.dataset_id.combined_ad_metrics` (
  platform,
  campaign_id,
  ad_conversion_rate,
  ad_roi,
  session_value,
  ad_engagement_rate,
  bounce_rate_per_ad_channel,
  cross_channel_conversion_rate,
  avg_session_duration_per_ad_channel,
  ad_cpa,
  date
)
SELECT
  ads.platform,
  ads.campaign_id,
  (SUM(ads.conversions) / SUM(ads.clicks)) * 100 AS ad_conversion_rate,
  (SUM(ga.revenue) / SUM(ads.cost)) * 100 AS ad_roi,
  (SUM(ga.revenue) / SUM(ga.sessions)) AS session_value,
  (SUM(ads.engagements) / SUM(ads.impressions)) * 100 AS ad_engagement_rate,
  (SUM(ga.single_page_sessions) / SUM(ga.sessions)) * 100 AS bounce_rate_per_ad_channel,
  (SUM(ads.conversions) / SUM(ads.clicks)) * 100 AS cross_channel_conversion_rate,
  (SUM(ga.total_session_duration) / SUM(ga.sessions)) AS avg_session_duration_per_ad_channel,
  (SUM(ads.cost) / SUM(ads.conversions)) AS ad_cpa,
  ads.date
FROM 
  `project_id.dataset_id.standardized_ads_data` AS ads
JOIN 
  `project_id.dataset_id.google_analytics_data` AS ga
ON 
  ads.date = ga.date AND ads.campaign_id = ga.campaign_id
GROUP BY 
  ads.platform, ads.campaign_id, ads.date;
```
To estimate the implications for size with quarter-hourly data aggregation, we need to adjust our calculations accordingly. Let’s go through the steps with this higher granularity.

Assumptions

1. Number of Campaigns: 20 campaigns per platform.
2. Number of Platforms: 4 platforms (Google Ads, Facebook Ads, Bing Ads, TikTok Ads).
3. Granularity: Quarter-hourly (every 15 minutes).
4. Number of Aggregation Periods per Day: 24 hours * 4 periods per hour = 96 periods per day.

Number of Rows per Day

+ Each campaign generates one row per period per platform.
+ For 20 campaigns on 4 platforms with 96 periods per day:

\text{Number of rows per day} = 20 \text{ campaigns} \times 4 \text{ platforms} \times 96 \text{ periods/day} = 7,680 \text{ rows/day}


Number of Rows per Week

+ Over a week (7 days):

\text{Number of rows per week} = 7,680 \text{ rows/day} \times 7 \text{ days} = 53,760 \text{ rows/week}


Columns and Row Size

1. Columns: The combined table has 10 columns.
2. Row Size Estimation:
+ String columns (platform, campaign_id): 20 bytes each.
+ Float columns (metrics like ad_conversion_rate, ad_roi, etc.): 8 bytes each.
+ Date column: 8 bytes.
+ Total row size:

20 \text{ bytes} \times 2 + 8 \text{ bytes} \times 7 + 8 \text{ bytes} = 96 \text{ bytes/row}


Total Storage Estimate for One Week

1. Number of Rows per Week: 53,760 rows
2. Total Storage:


53,760 \text{ rows} \times 96 \text{ bytes/row} = 5,161,600 \text{ bytes} = 5.16 \text{ MB}


Implications for Size of All Tables

1. Google Ads Data Table:
+ Rows per Week: 53,760
+ Columns: 15
+ Row Size: 15 \text{ columns} \times 50 \text{ bytes} = 750 \text{ bytes/row}
+ Weekly Storage: 53,760 \text{ rows} \times 750 \text{ bytes} = 40,320,000 \text{ bytes} = 40.32 \text{ MB}
2. Facebook Ads Data Table:
+ Rows per Week: 53,760
+ Columns: 15
+ Row Size: 750 bytes/row
+ Weekly Storage: 40.32 MB
3. Bing Ads Data Table:
+ Rows per Week: 53,760
+ Columns: 15
+ Row Size: 750 bytes/row
+ Weekly Storage: 40.32 MB
4. TikTok Ads Data Table:
+ Rows per Week: 53,760
+ Columns: 15
+ Row Size: 750 bytes/row
+ Weekly Storage: 40.32 MB
5. Google Analytics Data Table:
+ Rows per Week: 53,760
+ Columns: 15
+ Row Size: 750 bytes/row
+ Weekly Storage: 40.32 MB
6. Standardized Ads Data Table:
+ Rows per Week: 53,760
+ Columns: 15
+ Row Size: 750 bytes/row
+ Weekly Storage: 40.32 MB
7. Combined Metrics Table:
+ Rows per Week: 53,760
+ Columns: 10
+ Row Size: 96 bytes/row
+ Weekly Storage: 5.16 MB

Total Storage Estimate for All Tables for One Week

1. Google Ads Data Table: 40.32 MB
2. Facebook Ads Data Table: 40.32 MB
3. Bing Ads Data Table: 40.32 MB
4. TikTok Ads Data Table: 40.32 MB
5. Google Analytics Data Table: 40.32 MB
6. Standardized Ads Data Table: 40.32 MB
7. Combined Metrics Table: 5.16 MB

Total Weekly Storage: 6 \times 40.32 \text{ MB} + 5.16 \text{ MB} = 243.08 \text{ MB}

With quarter-hourly data aggregation, the total storage requirement for one week across all tables is approximately 243.08 MB.

The Standardized Ads Data Table serves as an intermediate table that consolidates and standardizes ad performance data from multiple advertising platforms (Google Ads, Facebook Ads, Bing Ads, TikTok Ads). The main purposes of this table are:

1. Data Consistency: Ensures that data from different sources is uniform in format and structure, making it easier to analyze and compare.
2. Unified Schema: Combines similar metrics (e.g., impressions, clicks, conversions) from different platforms into a common schema.
3. Simplified Analysis: Facilitates streamlined and consistent analysis by providing a single point of reference for all ad performance data.
4. Integration: Enables seamless integration with other datasets, such as Google Analytics data, to create comprehensive combined metrics.
5. Preprocessing: Acts as a preprocessing step that cleans and organizes raw data before further transformations and calculations are performed.

We’ll need to create view files for each table, a model file that includes these views, and a manifest file for the Data Product Explore and LookML Dashboard.
+ View Files: Define the schema and metrics for standardized_ads_data and combined_metrics.
+ Model File: Defines the explore for the data product, including the joins and relationships.
+ Manifest File: Defines the dashboard layout and elements, including filters and single value displays for key metrics.



